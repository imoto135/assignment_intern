# 時系列予測の王道アプローチ

## 📋 全体フロー

```
1. 問題設定 → 2. データ分割 → 3. ベースライン → 4. 特徴量エンジニアリング
   → 5. モデル選択 → 6. 評価・改善 → 7. 本番化
```

---

## 1️⃣ 問題設定の明確化

### 予測タスクの定義

**ETTデータセットの場合:**

```python
# 例1: 単一ステップ予測
入力: 過去24時間の負荷データ (t-24 ~ t-1)
出力: 1時間後のOT (t)

# 例2: 多ステップ予測（より実用的）
入力: 過去24時間の負荷データ (t-24 ~ t-1)
出力: 未来12時間のOT (t ~ t+11)

# 例3: 長期予測
入力: 過去168時間（1週間）のデータ
出力: 未来24時間のOT
```

### 評価指標の決定

| 指標     | 特徴                   | 使い分け                   |
| -------- | ---------------------- | -------------------------- |
| **MAE**  | 平均絶対誤差           | 外れ値に頑健、解釈しやすい |
| **RMSE** | 二乗平均平方根誤差     | 大きな誤差にペナルティ     |
| **MAPE** | 平均絶対パーセント誤差 | スケールに依存しない       |

**推奨**: MAE（主指標）+ RMSE（補助指標）

---

## 2️⃣ データ分割戦略 ⚠️ 最重要

### ❌ 絶対にやってはいけないこと

```python
# ランダム分割は時系列では NG！
X_train, X_test = train_test_split(X, y, test_size=0.2)  # ❌ データリーク！
```

### ✅ 正しい時系列分割

```python
# 時系列順に分割
# ├─────────────┼──────┼──────┤
# │   Train     │ Val  │ Test │
# └─────────────┴──────┴──────┘
#     70%        15%     15%

# ETTh1の例（17,420サンプル）
train_size = int(0.7 * len(df))  # 12,194
val_size = int(0.15 * len(df))   # 2,613
test_size = len(df) - train_size - val_size  # 2,613

train = df[:train_size]
val = df[train_size:train_size+val_size]
test = df[train_size+val_size:]
```

### 時系列交差検証（より堅牢）

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
# Fold 1: ├─train─┤─test─┤
# Fold 2: ├──train──┤─test─┤
# Fold 3: ├────train────┤─test─┤
# ...
```

---

## 3️⃣ ベースラインモデルの構築

**必ず最初に簡単なモデルで基準を作る！**

### ベースライン1: 単純な予測

```python
# 1. 前の値をそのまま使う（Naive forecast）
y_pred = y_train[-1]  # 最後の値を予測値とする

# 2. 移動平均
y_pred = y_train[-24:].mean()  # 過去24時間の平均

# 3. 季節性を考慮
y_pred = y_train[-24]  # 24時間前の値（日次周期性）
```

### ベースライン2: 統計モデル

```python
# ARIMA（自己回帰和分移動平均）
from statsmodels.tsa.arima.model import ARIMA
model = ARIMA(train['OT'], order=(24, 1, 0))
model_fit = model.fit()
```

**目的**: これを超えられなければ複雑なモデルは不要！

---

## 4️⃣ 特徴量エンジニアリング

### 時間的特徴

```python
# 周期的な特徴
df['hour'] = df.index.hour
df['day_of_week'] = df.index.dayofweek
df['month'] = df.index.month
df['is_weekend'] = df.index.dayofweek.isin([5, 6]).astype(int)

# 周期性をサイン・コサインで表現（連続性を保つ）
df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)
df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24)
df['day_sin'] = np.sin(2 * np.pi * df.index.dayofyear / 365)
df['day_cos'] = np.cos(2 * np.pi * df.index.dayofyear / 365)
```

### ラグ特徴（過去の値）

```python
# 過去の値を特徴量として使う
for lag in [1, 2, 3, 6, 12, 24, 48, 168]:  # 1h, 2h, 3h, 6h, 12h, 1d, 2d, 1w
    df[f'OT_lag_{lag}'] = df['OT'].shift(lag)
    df[f'HUFL_lag_{lag}'] = df['HUFL'].shift(lag)
```

### 移動統計量

```python
# 移動平均・標準偏差
for window in [6, 12, 24]:
    df[f'OT_rolling_mean_{window}'] = df['OT'].rolling(window).mean()
    df[f'OT_rolling_std_{window}'] = df['OT'].rolling(window).std()
    df[f'OT_rolling_min_{window}'] = df['OT'].rolling(window).min()
    df[f'OT_rolling_max_{window}'] = df['OT'].rolling(window).max()
```

### ドメイン知識に基づく特徴

```python
# 皮相電力（実際に流れる電力）
df['H_apparent'] = np.sqrt(df['HUFL']**2 + df['HULL']**2)
df['M_apparent'] = np.sqrt(df['MUFL']**2 + df['MULL']**2)
df['L_apparent'] = np.sqrt(df['LUFL']**2 + df['LULL']**2)

# 力率（効率の指標）
df['H_power_factor'] = df['HUFL'] / (df['H_apparent'] + 1e-8)

# 総負荷
df['total_useful'] = df['HUFL'] + df['MUFL'] + df['LUFL']
df['total_useless'] = df['HULL'] + df['MULL'] + df['LULL']

# 負荷の変化率
df['OT_diff'] = df['OT'].diff()
df['HUFL_diff'] = df['HUFL'].diff()
```

---

## 5️⃣ モデル選択

### レベル1: 機械学習モデル（まずはここから）

```python
# 1. LightGBM（勾配ブースティング）- 最も実用的
import lightgbm as lgb

model = lgb.LGBMRegressor(
    n_estimators=1000,
    learning_rate=0.01,
    max_depth=7,
    num_leaves=31,
    random_state=42
)

# 2. XGBoost
import xgboost as xgb
model = xgb.XGBRegressor(...)

# 3. Random Forest
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(...)
```

**特徴**:

- ✅ 実装が簡単
- ✅ 解釈性が高い（特徴量重要度）
- ✅ 過学習しにくい
- ❌ 長期予測は苦手

### レベル2: ディープラーニング（より高度）

```python
# 1. LSTM（Long Short-Term Memory）
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(24, 7)),
    LSTM(64),
    Dense(32, activation='relu'),
    Dense(1)
])

# 2. Transformer（最新）
# - Informer（ETTデータセット用に開発されたモデル）
# - Autoformer
# - FEDformer
```

**特徴**:

- ✅ 長期依存関係を捉えられる
- ✅ 多変量時系列に強い
- ❌ データ量が必要
- ❌ 学習に時間がかかる

### レベル3: 最新の時系列専用モデル

```python
# 1. N-BEATS（ニューラルベース）
# 2. TFT（Temporal Fusion Transformer）
# 3. TimeGPT（大規模言語モデルの時系列版）
```

---

## 6️⃣ 評価と改善

### 評価のポイント

```python
# 1. 複数の指標で評価
from sklearn.metrics import mean_absolute_error, mean_squared_error

mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

# 2. 予測ホライズン別の評価
# 1時間後、6時間後、24時間後で精度が異なる
for h in [1, 6, 12, 24]:
    mae_h = mean_absolute_error(y_true[:, h-1], y_pred[:, h-1])
    print(f'{h}時間後のMAE: {mae_h:.3f}')

# 3. 可視化
plt.figure(figsize=(15, 5))
plt.plot(y_true, label='実測値', alpha=0.7)
plt.plot(y_pred, label='予測値', alpha=0.7)
plt.legend()
plt.title('予測結果の比較')
```

### よくある問題と対策

| 問題                     | 原因                 | 対策                       |
| ------------------------ | -------------------- | -------------------------- |
| 訓練は良いがテストで悪い | 過学習               | 正則化、Early Stopping     |
| 予測が遅れる（ラグ）     | ラグ特徴に依存しすぎ | 外生変数を活用             |
| 急激な変化を捉えられない | 平滑化しすぎ         | 差分特徴、アテンション機構 |

---

## 7️⃣ ETTデータセット向け推奨アプローチ

### 🎯 初級者向け（まずはここから）

```python
# ステップ1: LightGBM + 基本的な特徴量
# - ラグ特徴（1, 6, 12, 24時間）
# - 移動平均（6, 12, 24時間）
# - 時間的特徴（hour, day_of_week）
# - 皮相電力

# 期待精度: MAE 2.0-3.0℃程度
```

### 🎯 中級者向け

```python
# ステップ2: LSTM + 豊富な特徴量
# - 過去24-168時間のシーケンス
# - 多変量入力（全センサー）
# - アテンション機構

# 期待精度: MAE 1.5-2.0℃程度
```

### 🎯 上級者向け（論文レベル）

```python
# ステップ3: Transformer系モデル
# - Informer（ETT用に開発）
# - アンサンブル（複数モデルの組み合わせ）

# 期待精度: MAE 1.0-1.5℃程度
```

---

## 📚 実装の順序（推奨）

### Week 1: 基礎固め

1. ✅ EDA完了（済）
2. データ分割の実装
3. ベースラインモデル（Naive, 移動平均）
4. 評価関数の実装

### Week 2: 機械学習

5. 基本的な特徴量エンジニアリング
6. LightGBM/XGBoostの実装
7. ハイパーパラメータチューニング
8. 特徴量重要度の分析

### Week 3: 高度化（オプション）

9. LSTM/Transformerの実装
10. アンサンブル
11. 最終評価とレポート作成

---

## 🔑 成功のポイント

1. **ベースラインを必ず作る** - 複雑なモデルの前に簡単なモデルで基準を
2. **データリークに注意** - 未来の情報を使わない
3. **ドメイン知識を活用** - 電力工学の知識を特徴量に反映
4. **可視化を重視** - 予測結果を必ず目で確認
5. **段階的に改善** - 一度に全部やらず、1つずつ検証

---

## 📖 参考文献

- **論文**: "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting" (AAAI 2021)
- **書籍**: 「時系列解析」（沖本竜義）
- **ライブラリ**:
  - `statsmodels`: 統計モデル
  - `prophet`: Facebook製時系列予測
  - `darts`: 時系列予測の統合ライブラリ
